{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip3 -qq install bokeh==0.13.0\n",
    "!pip3 -qq install gensim==3.6.0\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\vladi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\vladi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'.', 'ADJ', 'ADP', 'NUM', 'PRT', 'X', 'DET', 'PRON', 'CONJ', 'NOUN', 'VERB', 'ADV'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vladi\\AppData\\Local\\Temp\\ipykernel_20512\\3805302136.py:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vladi\\AppData\\Local\\Temp\\ipykernel_20512\\648162845.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vladi\\AppData\\Local\\Temp\\ipykernel_20512\\28852427.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim,lstm_layers_count, bidirectional=False)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.emb(inputs)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.141\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "mask = (y_batch != 0).float()\n",
    "preds = torch.argmax(logits,2)\n",
    "current_count = ((preds ==y_batch).float() * mask).sum()\n",
    "total_count = mask.sum()\n",
    "accuracy = current_count / total_count\n",
    "print('Accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.54\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(logits.transpose(2,1), y_batch)\n",
    "print('Loss: {:.3}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.transpose(2,1), y_batch)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                mask = (y_batch != 0).float()\n",
    "                preds = torch.argmax(logits,2)\n",
    "                cur_correct_count = ((preds ==y_batch).float() * mask).sum()\n",
    "                cur_sum_count = mask.sum()\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 15] Train: Loss = 0.32338, Accuracy = 71.24%: 100%|██████████| 572/572 [00:10<00:00, 54.56it/s]\n",
      "[1 / 15]   Val: Loss = 0.10220, Accuracy = 84.91%: 100%|██████████| 13/13 [00:00<00:00, 21.44it/s]\n",
      "[2 / 15] Train: Loss = 0.10132, Accuracy = 89.93%: 100%|██████████| 572/572 [00:09<00:00, 58.82it/s]\n",
      "[2 / 15]   Val: Loss = 0.07484, Accuracy = 89.46%: 100%|██████████| 13/13 [00:00<00:00, 22.40it/s]\n",
      "[3 / 15] Train: Loss = 0.06710, Accuracy = 93.18%: 100%|██████████| 572/572 [00:10<00:00, 56.03it/s]\n",
      "[3 / 15]   Val: Loss = 0.06608, Accuracy = 91.08%: 100%|██████████| 13/13 [00:00<00:00, 24.02it/s]\n",
      "[4 / 15] Train: Loss = 0.05129, Accuracy = 94.79%: 100%|██████████| 572/572 [00:09<00:00, 57.26it/s]\n",
      "[4 / 15]   Val: Loss = 0.06605, Accuracy = 91.92%: 100%|██████████| 13/13 [00:00<00:00, 23.05it/s]\n",
      "[5 / 15] Train: Loss = 0.04088, Accuracy = 95.82%: 100%|██████████| 572/572 [00:08<00:00, 63.63it/s]\n",
      "[5 / 15]   Val: Loss = 0.06106, Accuracy = 92.45%: 100%|██████████| 13/13 [00:00<00:00, 23.86it/s]\n",
      "[6 / 15] Train: Loss = 0.03326, Accuracy = 96.55%: 100%|██████████| 572/572 [00:08<00:00, 67.18it/s]\n",
      "[6 / 15]   Val: Loss = 0.06615, Accuracy = 92.81%: 100%|██████████| 13/13 [00:00<00:00, 24.21it/s]\n",
      "[7 / 15] Train: Loss = 0.02739, Accuracy = 97.14%: 100%|██████████| 572/572 [00:08<00:00, 67.78it/s]\n",
      "[7 / 15]   Val: Loss = 0.07239, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 25.75it/s]\n",
      "[8 / 15] Train: Loss = 0.02279, Accuracy = 97.63%: 100%|██████████| 572/572 [00:08<00:00, 67.20it/s]\n",
      "[8 / 15]   Val: Loss = 0.06761, Accuracy = 93.09%: 100%|██████████| 13/13 [00:00<00:00, 24.75it/s]\n",
      "[9 / 15] Train: Loss = 0.01887, Accuracy = 98.04%: 100%|██████████| 572/572 [00:08<00:00, 67.43it/s]\n",
      "[9 / 15]   Val: Loss = 0.06889, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 23.38it/s]\n",
      "[10 / 15] Train: Loss = 0.01565, Accuracy = 98.37%: 100%|██████████| 572/572 [00:08<00:00, 66.59it/s]\n",
      "[10 / 15]   Val: Loss = 0.06878, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 23.26it/s]\n",
      "[11 / 15] Train: Loss = 0.01300, Accuracy = 98.67%: 100%|██████████| 572/572 [00:08<00:00, 66.97it/s]\n",
      "[11 / 15]   Val: Loss = 0.07297, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 23.24it/s]\n",
      "[12 / 15] Train: Loss = 0.01073, Accuracy = 98.93%: 100%|██████████| 572/572 [00:08<00:00, 67.29it/s]\n",
      "[12 / 15]   Val: Loss = 0.07700, Accuracy = 92.97%: 100%|██████████| 13/13 [00:00<00:00, 23.99it/s]\n",
      "[13 / 15] Train: Loss = 0.00867, Accuracy = 99.14%: 100%|██████████| 572/572 [00:08<00:00, 67.12it/s]\n",
      "[13 / 15]   Val: Loss = 0.07791, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 23.18it/s]\n",
      "[14 / 15] Train: Loss = 0.00701, Accuracy = 99.32%: 100%|██████████| 572/572 [00:08<00:00, 66.83it/s]\n",
      "[14 / 15]   Val: Loss = 0.08311, Accuracy = 92.86%: 100%|██████████| 13/13 [00:00<00:00, 23.43it/s]\n",
      "[15 / 15] Train: Loss = 0.00578, Accuracy = 99.46%: 100%|██████████| 572/572 [00:08<00:00, 66.79it/s] \n",
      "[15 / 15]   Val: Loss = 0.08839, Accuracy = 92.85%: 100%|██████████| 13/13 [00:00<00:00, 23.39it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=15,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0217\n",
      "Loss: 2.58\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "mask = (y_batch != 0).float()\n",
    "preds = torch.argmax(logits,2)\n",
    "current_count = ((preds ==y_batch).float() * mask).sum()\n",
    "total_count = mask.sum()\n",
    "accuracy = current_count / total_count\n",
    "print('Accuracy: {:.3}'.format(accuracy))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "loss = criterion(logits.transpose(2,1), y_batch)\n",
    "print('Loss: {:.3}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # <create layers>\n",
    "        self.emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # <apply them>\n",
    "        out = self.emb(inputs)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.55535, Accuracy = 82.58%: 100%|██████████| 572/572 [00:15<00:00, 37.30it/s]\n",
      "[1 / 20]   Val: Loss = 0.27014, Accuracy = 91.20%: 100%|██████████| 13/13 [00:01<00:00,  8.24it/s]\n",
      "[2 / 20] Train: Loss = 0.20458, Accuracy = 93.50%: 100%|██████████| 572/572 [00:14<00:00, 38.77it/s]\n",
      "[2 / 20]   Val: Loss = 0.17697, Accuracy = 94.34%: 100%|██████████| 13/13 [00:00<00:00, 13.77it/s]\n",
      "[3 / 20] Train: Loss = 0.13017, Accuracy = 95.95%: 100%|██████████| 572/572 [00:14<00:00, 38.91it/s]\n",
      "[3 / 20]   Val: Loss = 0.13770, Accuracy = 95.56%: 100%|██████████| 13/13 [00:01<00:00, 12.38it/s]\n",
      "[4 / 20] Train: Loss = 0.08884, Accuracy = 97.28%: 100%|██████████| 572/572 [00:14<00:00, 39.53it/s]\n",
      "[4 / 20]   Val: Loss = 0.11981, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 13.06it/s]\n",
      "[5 / 20] Train: Loss = 0.06170, Accuracy = 98.16%: 100%|██████████| 572/572 [00:14<00:00, 38.20it/s]\n",
      "[5 / 20]   Val: Loss = 0.11270, Accuracy = 96.41%: 100%|██████████| 13/13 [00:01<00:00, 12.27it/s]\n",
      "[6 / 20] Train: Loss = 0.04211, Accuracy = 98.78%: 100%|██████████| 572/572 [00:14<00:00, 39.92it/s] \n",
      "[6 / 20]   Val: Loss = 0.11090, Accuracy = 96.58%: 100%|██████████| 13/13 [00:01<00:00, 12.30it/s]\n",
      "[7 / 20] Train: Loss = 0.02843, Accuracy = 99.22%: 100%|██████████| 572/572 [00:14<00:00, 39.19it/s]\n",
      "[7 / 20]   Val: Loss = 0.11353, Accuracy = 96.61%: 100%|██████████| 13/13 [00:01<00:00, 11.94it/s]\n",
      "[8 / 20] Train: Loss = 0.01879, Accuracy = 99.52%: 100%|██████████| 572/572 [00:15<00:00, 37.51it/s] \n",
      "[8 / 20]   Val: Loss = 0.11907, Accuracy = 96.70%: 100%|██████████| 13/13 [00:01<00:00, 12.64it/s]\n",
      "[9 / 20] Train: Loss = 0.01198, Accuracy = 99.73%: 100%|██████████| 572/572 [00:14<00:00, 39.86it/s] \n",
      "[9 / 20]   Val: Loss = 0.12858, Accuracy = 96.64%: 100%|██████████| 13/13 [00:01<00:00, 12.51it/s]\n",
      "[10 / 20] Train: Loss = 0.00770, Accuracy = 99.84%: 100%|██████████| 572/572 [00:14<00:00, 39.12it/s] \n",
      "[10 / 20]   Val: Loss = 0.13088, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 13.24it/s]\n",
      "[11 / 20] Train: Loss = 0.00469, Accuracy = 99.92%: 100%|██████████| 572/572 [00:14<00:00, 39.78it/s] \n",
      "[11 / 20]   Val: Loss = 0.13700, Accuracy = 96.64%: 100%|██████████| 13/13 [00:01<00:00, 11.90it/s]\n",
      "[12 / 20] Train: Loss = 0.00310, Accuracy = 99.96%: 100%|██████████| 572/572 [00:14<00:00, 40.85it/s] \n",
      "[12 / 20]   Val: Loss = 0.14790, Accuracy = 96.64%: 100%|██████████| 13/13 [00:01<00:00, 12.29it/s]\n",
      "[13 / 20] Train: Loss = 0.00182, Accuracy = 99.98%: 100%|██████████| 572/572 [00:14<00:00, 38.86it/s] \n",
      "[13 / 20]   Val: Loss = 0.15397, Accuracy = 96.72%: 100%|██████████| 13/13 [00:01<00:00, 12.27it/s]\n",
      "[14 / 20] Train: Loss = 0.00145, Accuracy = 99.98%: 100%|██████████| 572/572 [00:14<00:00, 40.00it/s] \n",
      "[14 / 20]   Val: Loss = 0.16972, Accuracy = 96.55%: 100%|██████████| 13/13 [00:01<00:00, 12.87it/s]\n",
      "[15 / 20] Train: Loss = 0.00153, Accuracy = 99.98%: 100%|██████████| 572/572 [00:13<00:00, 40.95it/s] \n",
      "[15 / 20]   Val: Loss = 0.17524, Accuracy = 96.51%: 100%|██████████| 13/13 [00:01<00:00, 11.89it/s]\n",
      "[16 / 20] Train: Loss = 0.00328, Accuracy = 99.91%: 100%|██████████| 572/572 [00:14<00:00, 40.06it/s] \n",
      "[16 / 20]   Val: Loss = 0.17691, Accuracy = 96.55%: 100%|██████████| 13/13 [00:01<00:00, 12.54it/s]\n",
      "[17 / 20] Train: Loss = 0.00160, Accuracy = 99.97%: 100%|██████████| 572/572 [00:14<00:00, 40.52it/s] \n",
      "[17 / 20]   Val: Loss = 0.17166, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 13.46it/s]\n",
      "[18 / 20] Train: Loss = 0.00064, Accuracy = 99.99%: 100%|██████████| 572/572 [00:14<00:00, 39.98it/s] \n",
      "[18 / 20]   Val: Loss = 0.17903, Accuracy = 96.68%: 100%|██████████| 13/13 [00:01<00:00, 12.98it/s]\n",
      "[19 / 20] Train: Loss = 0.00031, Accuracy = 100.00%: 100%|██████████| 572/572 [00:14<00:00, 39.19it/s]\n",
      "[19 / 20]   Val: Loss = 0.18054, Accuracy = 96.78%: 100%|██████████| 13/13 [00:01<00:00, 12.07it/s]\n",
      "[20 / 20] Train: Loss = 0.00022, Accuracy = 100.00%: 100%|██████████| 572/572 [00:14<00:00, 38.73it/s]\n",
      "[20 / 20]   Val: Loss = 0.18510, Accuracy = 96.76%: 100%|██████████| 13/13 [00:01<00:00, 12.00it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.key_to_index:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # <create layers>\n",
    "        self.emb = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # <apply them>\n",
    "        out = self.emb(inputs)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.46690, Accuracy = 86.23%: 100%|██████████| 572/572 [00:11<00:00, 49.25it/s]\n",
      "[1 / 20]   Val: Loss = 0.22562, Accuracy = 93.28%: 100%|██████████| 13/13 [00:01<00:00, 12.82it/s]\n",
      "[2 / 20] Train: Loss = 0.16370, Accuracy = 95.11%: 100%|██████████| 572/572 [00:11<00:00, 50.35it/s]\n",
      "[2 / 20]   Val: Loss = 0.15414, Accuracy = 95.29%: 100%|██████████| 13/13 [00:01<00:00, 12.02it/s]\n",
      "[3 / 20] Train: Loss = 0.11623, Accuracy = 96.50%: 100%|██████████| 572/572 [00:11<00:00, 49.92it/s]\n",
      "[3 / 20]   Val: Loss = 0.12694, Accuracy = 96.06%: 100%|██████████| 13/13 [00:01<00:00, 13.00it/s]\n",
      "[4 / 20] Train: Loss = 0.09318, Accuracy = 97.16%: 100%|██████████| 572/572 [00:11<00:00, 50.63it/s]\n",
      "[4 / 20]   Val: Loss = 0.11332, Accuracy = 96.40%: 100%|██████████| 13/13 [00:01<00:00, 12.83it/s]\n",
      "[5 / 20] Train: Loss = 0.07923, Accuracy = 97.58%: 100%|██████████| 572/572 [00:11<00:00, 50.63it/s]\n",
      "[5 / 20]   Val: Loss = 0.10374, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 13.05it/s]\n",
      "[6 / 20] Train: Loss = 0.06923, Accuracy = 97.88%: 100%|██████████| 572/572 [00:11<00:00, 51.08it/s]\n",
      "[6 / 20]   Val: Loss = 0.09843, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 13.37it/s]\n",
      "[7 / 20] Train: Loss = 0.06147, Accuracy = 98.11%: 100%|██████████| 572/572 [00:11<00:00, 50.79it/s]\n",
      "[7 / 20]   Val: Loss = 0.09618, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 13.07it/s]\n",
      "[8 / 20] Train: Loss = 0.05520, Accuracy = 98.29%: 100%|██████████| 572/572 [00:11<00:00, 50.56it/s]\n",
      "[8 / 20]   Val: Loss = 0.09353, Accuracy = 97.00%: 100%|██████████| 13/13 [00:01<00:00, 12.52it/s]\n",
      "[9 / 20] Train: Loss = 0.04949, Accuracy = 98.47%: 100%|██████████| 572/572 [00:11<00:00, 50.66it/s]\n",
      "[9 / 20]   Val: Loss = 0.09210, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 13.08it/s]\n",
      "[10 / 20] Train: Loss = 0.04484, Accuracy = 98.62%: 100%|██████████| 572/572 [00:11<00:00, 50.95it/s]\n",
      "[10 / 20]   Val: Loss = 0.09348, Accuracy = 97.05%: 100%|██████████| 13/13 [00:01<00:00, 12.84it/s]\n",
      "[11 / 20] Train: Loss = 0.04048, Accuracy = 98.75%: 100%|██████████| 572/572 [00:11<00:00, 50.76it/s]\n",
      "[11 / 20]   Val: Loss = 0.09569, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 13.21it/s]\n",
      "[12 / 20] Train: Loss = 0.03623, Accuracy = 98.89%: 100%|██████████| 572/572 [00:11<00:00, 50.62it/s]\n",
      "[12 / 20]   Val: Loss = 0.09527, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 13.30it/s]\n",
      "[13 / 20] Train: Loss = 0.03215, Accuracy = 99.03%: 100%|██████████| 572/572 [00:11<00:00, 50.89it/s]\n",
      "[13 / 20]   Val: Loss = 0.09791, Accuracy = 97.06%: 100%|██████████| 13/13 [00:01<00:00, 12.62it/s]\n",
      "[14 / 20] Train: Loss = 0.02870, Accuracy = 99.15%: 100%|██████████| 572/572 [00:11<00:00, 50.81it/s]\n",
      "[14 / 20]   Val: Loss = 0.10169, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 13.12it/s]\n",
      "[15 / 20] Train: Loss = 0.02537, Accuracy = 99.27%: 100%|██████████| 572/572 [00:11<00:00, 50.95it/s]\n",
      "[15 / 20]   Val: Loss = 0.10200, Accuracy = 97.02%: 100%|██████████| 13/13 [00:01<00:00, 12.91it/s]\n",
      "[16 / 20] Train: Loss = 0.02241, Accuracy = 99.37%: 100%|██████████| 572/572 [00:11<00:00, 51.13it/s]\n",
      "[16 / 20]   Val: Loss = 0.10614, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 13.04it/s]\n",
      "[17 / 20] Train: Loss = 0.01924, Accuracy = 99.47%: 100%|██████████| 572/572 [00:11<00:00, 50.90it/s] \n",
      "[17 / 20]   Val: Loss = 0.10708, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 13.31it/s]\n",
      "[18 / 20] Train: Loss = 0.01659, Accuracy = 99.57%: 100%|██████████| 572/572 [00:11<00:00, 50.91it/s] \n",
      "[18 / 20]   Val: Loss = 0.11046, Accuracy = 96.91%: 100%|██████████| 13/13 [00:01<00:00, 12.88it/s]\n",
      "[19 / 20] Train: Loss = 0.01420, Accuracy = 99.64%: 100%|██████████| 572/572 [00:11<00:00, 50.82it/s] \n",
      "[19 / 20]   Val: Loss = 0.11445, Accuracy = 96.80%: 100%|██████████| 13/13 [00:01<00:00, 12.89it/s]\n",
      "[20 / 20] Train: Loss = 0.01207, Accuracy = 99.72%: 100%|██████████| 572/572 [00:11<00:00, 51.24it/s] \n",
      "[20 / 20]   Val: Loss = 0.11871, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 13.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=torch.FloatTensor(embeddings),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0435\n",
      "Loss : 2.57\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=torch.FloatTensor(embeddings),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "logits = model(X_batch)\n",
    "\n",
    "# <calc accuracy>\n",
    "mask = (y_batch != 0).float()\n",
    "preds = torch.argmax(logits, 2)\n",
    "current_count = ((preds == y_batch).float() * mask).sum()\n",
    "total_count = mask.sum()\n",
    "accuracy = current_count / total_count\n",
    "print('Accuracy: {:.3}'.format(accuracy))\n",
    "\n",
    "#<calc loss>\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = criterion(logits.transpose(2, 1), y_batch)\n",
    "print('Loss : {:.3}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected TensorOptions(dtype=float, device=cuda, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(sentence_embs, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtype(FloatTensor)\n\u001b[1;32m---> 18\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     20\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m LongTensor(sentence_tags))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mTypeError\u001b[0m: expected TensorOptions(dtype=float, device=cuda, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for sentence in test_data:\n",
    "    sentence_embs = []\n",
    "    sentence_tags = []\n",
    "    \n",
    "    for word, tag in sentence:\n",
    "        try:\n",
    "            word_emb = torch.from_numpy(w2v_model.get_vector(word.lower())).float()\n",
    "        except:\n",
    "            word_emb = torch.zeros(w2v_model.vectors.shape[1]).float()\n",
    "        sentence_embs.append(word_emb)\n",
    "        sentence_tags.append(tag2ind[tag])\n",
    "        \n",
    "    embeddings = torch.cat(sentence_embs, 0)\n",
    "    model = model.type(FloatTensor)\n",
    "    logits = model(FloatTensor(embeddings))\n",
    "    preds = torch.argmax(logits, 2)\n",
    "    correct += (preds == LongTensor(sentence_tags)).float().sum()\n",
    "    total += len(tags)\n",
    "    \n",
    "\n",
    "accuracy = correct / total\n",
    "print('Accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
